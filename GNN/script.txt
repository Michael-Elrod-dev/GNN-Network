In recent years, we've seen increasing deployment of autonomous drone fleets for many different applications like
disaster response, environmental monitoring and surveillance. But effectively coordinating these multi-drone systems
presents several significant challenges.
First, these drones operate under partial observability - they can only see what's within their limited field of view,
making it difficult to develop a complete understanding of the environment. This is particularly challenging when the
location of their objective isn't known in advance for example in many search and rescue efforts.
Second, drones have strict energy constraints which limit their communication range. They can't simply broadcast
high-resolution data to every other drone in the fleet. Instead, they must rely on lightweight communication protocols
with only their nearest neighbors or by utilizing some centralized communication hub.
Third, these drones often operate in uncertain and dynamic environments where conditions can change presenting unique
challenges.
Traditional path-planning algorithms like traveling salesman problem solvers and greedy algorithms struggle under these
constraints, particularly when prior information about the environment or objective locations aren’t available. This is
the gap my research addresses with a framework that integrates a Graph Neural Network, an attention transformer, and deep
reinforcement learning to enable robust coordination even under these challenging conditions.

To further break down why existing path-planning methods struggle with this problem:
First, we have Traveling Salesman Problem or TSP solvers. While mathematically elegant, they assume complete knowledge
of all target locations in advance. In real disaster scenarios or environmental monitoring missions, this information
simply isn't available beforehand. Numerical optimization techniques face similar challenges. They can compute optimal
paths efficiently, but require well-defined objectives and constraints that are often impossible to formulate in
uncertain environments. Greedy algorithms are also computationally light and work well for single agents, but they
struggle with multi-agent coordination. When multiple drones make locally optimal decisions without coordination, we see
significant path redundancy and inefficient resource utilization.
Even standard Deep Reinforcement Learning approaches face difficulties in multi-agent settings. They often scale poorly
as the number of agents increases and struggle to leverage the benefits of coordination when communication is limited.
For this solution I needed an approach that could handle partial observability, enable efficient information exchange
between agents, and adapt to dynamic environments while maintaining scalability. This is what motivated the integrated
GNN-Transformer-Deep learning framework.

At its core, my approach models the multi-agent system as a dynamic graph where both agents and goals are represented as
nodes, and their relationships as weighted edges. This graph structure is constantly updated as agents move and discover
new information.
The Graph Neural Network serves as the foundation, enabling agents to efficiently share information with their neighbors
through a message-passing mechanism. Instead of transmitting raw sensor data, agents share compact structured state
features — their positions and goal assignments — which the GNN then processes locally into rich representations for
decision-making.
This is enhanced with a transformer-based architecture that leverages attention mechanisms to prioritize the most
relevant interactions. This allows drones to focus on objectives and important coordination opportunities, rather than
processing all available information equally.
Finally, I optimize agent policies using a Deep Q-Network with prioritized experience replay, which helps agents learn
effective decision-making strategies even in partially observable environments.
What makes this approach particularly powerful is how these three components work together: the GNN provides efficient
local information exchange, the transformer captures complex relational patterns from that shared information, and the
DQN optimizes decision-making based on this enriched information. This integration enables robust coordination even
under strict communication constraints and partial observability.

To evaluate my approach, I developed a grid-based environment that captures the key challenges of multi-agent
coordination under partial observability and limited communication.
The environment consists of an L×L grid where L represents the dimensions of our environment. As you can see in this
visualization, we have N autonomous agents—represented by these drone icons—that must cooperatively discover and serve
M goals distributed throughout the grid–represented by the green dots.
Each agent has two key constraints that model real-world limitations. First, they have a limited vision radius of 4.5
grid units, shown by these red circles. Agents can only perceive goals within this radius, creating partial observability.
Second, each agent can only communicate with its k nearest neighbors. This models the bandwidth and range limitations
of real drone communication systems. These communication links form a dynamic, bidirectional graph that evolves as
agents move through the environment.
When a goal falls within an agent's visual range it becomes part of the shared knowledge. However, a goal is only
considered 'served' when an agent physically enters its cell represented by the grey dots that agents have passed over
already. These goals are then no longer considered in the process.
At the beginning of each episode, agents are distributed along the four borders of the environment, simulating realistic
deployment scenarios where drone stations are typically located outside the coverage area. The goals are randomly
distributed throughout the interior of the grid, with their locations unknown to the agents until discovery.
This setup creates a challenging scenario that requires efficient exploration, information sharing, and coordination—the
key aspects my approach aims to address.

The foundation of the approach is a specialized Graph Neural Network architecture designed to process and leverage the
relationships between agents and goals in our environment.
As shown in this diagram, the GNN consists of two primary components: an entity-specific embedding layer and an attention
transformer.
The embedding layer processes raw node features, distinguishing between agents and goals while incorporating spatial
relationships. Each node—whether an agent or goal—is represented by a feature vector that captures its position and
entity type, while agent nodes also include information regarding their relationship to other nearby nodes.
Next we leverage multi-head transformer layers with edge-feature-enhanced attention. This allows agents to weigh the
importance of different connections dynamically.
When an agent receives information from its neighbors, the attention mechanism helps determine which information is
most relevant for decision-making. For example, an agent might give higher priority to information about nearby
uncollected goals or to agents that have discovered new regions of the environment.
The output from this GNN forms a semantic embedding that captures both the local environment state and the collaborative
information shared across the agent network. These embeddings then feed into our Deep Q-Network, which uses them to
determine optimal actions. What makes this architecture particularly powerful is its ability to maintain performance
even with limited communication bandwidth, as agents share compact structured state features rather than raw sensory
data, keeping communication overhead low.

A critical component in the approach is how it dynamically constructs and updates the graph structure at each time step.
This process ensures that our graph captures the most relevant relationships between agents and goals.
I use a distance-based thresholding mechanism to determine which connections to maintain. For each agent, I compute
edge weights based on the Euclidean distance to other entities, but only if those connections satisfy three key constraints.
First, for agent-to-agent connections, I limit each agent to communicating with only its k nearest neighbors within a
distance threshold based on the size of the environment. This reflects the bandwidth limitations of real-world drone
communication systems and keeps our graph sparse.
Second, I apply another distance threshold where connections to goal locations are only valid if they're within the
agent's vision radius of 4.5 units. This ensures that agents only consider objectives they can actually perceive.
Third, I enforce non-self-connectivity, meaning agents don't form edges with themselves, which would of course be
redundant for information exchange.
The result is a dynamically evolving graph where edges are constantly being formed and broken as agents move through
the environment. When agents are close to each other, they form dense local networks for efficient information sharing.
As they spread out to explore, the graph adapts to maintain connectivity while focusing on the most important relationships.
This adaptive approach is crucial for scaling the framework to larger environments with many agents and goals, as it
prevents communication overload while preserving the essential information flow for effective coordination.

At the heart of the framework's ability to prioritize important information is the attention transformer. This is what
allows the system to focus on the most relevant interactions in a complex multi-agent environment.
I implement this using multi-head transformer layers with 3 attention heads. Each head can focus on different aspects
of the relationships between nodes, providing the model with multiple perspectives on the same graph structure.
The attention mechanism works by computing how important each connection is within the graph. Unlike standard approaches,
my method incorporates both information about the nodes themselves and about the quality of the connections between
them. This allows agents to prioritize relationships that are more relevant to their decision-making.
This is visualized in the heatmap you see here, where brighter colors indicate stronger attention weights between agents
and goals. For example, you can see that Agent 2 assigns the highest attention weight of 1.0 to Goal 4, indicating that
this goal is its highest priority.
What makes this approach distinctive is that it considers not just which nodes are important, but also how those nodes
are connected. This is crucial for effective coordination because it enables agents to filter out noise or redundancy
and focus on the most important information, especially in bandwidth-constrained environments where efficient
communication is essential.

Lastly, To optimize the actual decision-making process of our agents, I employ a Deep Q-Network framework enhanced with
prioritized experience replay—a combination that provides both stability and efficient learning in a partially observable
environment.
The DQN architecture uses two networks: an online network for action selection and a separate target network for value
estimation. This separation helps prevent the overestimation of action values, a common issue in standard DQN
implementations that can lead to suboptimal policies.
The Q-network is trained to minimize the temporal difference error between the predicted action-value and the target
value, which is computed using the reward plus the discounted future return. My loss function incorporates importance
sampling weights from our prioritized replay mechanism, giving more learning emphasis to rare but informative experiences.
Speaking of the prioritized experience replay, this is a crucial component that dramatically improves sample efficiency.
Instead of randomly sampling from our experience buffer, I prioritize transitions based on their temporal difference
error—essentially focusing our learning on the experiences that are the most unique or valuable.
For training, I use carefully selected hyperparameters: a learning rate of 0.0005, which provides a good balance between
convergence speed and stability; a replay buffer size of 100,000 transitions; and a soft update rate of 0.001 for
gradually transferring weights from the online to the target network.
During training, I use an epsilon-greedy exploration strategy where the exploration rate decays linearly from 1.0 to 0.01,
allowing our agents to transition smoothly from exploration to exploitation as they gain experience.
This learning algorithm, when combined with the GNN architecture and attention mechanism, enables our agents to develop
sophisticated coordination strategies that, as you will see, far outperform traditional approaches.

To thoroughly evaluate my approach, I designed a comprehensive set of experiments across different environment
configurations, scaling from simple scenarios to highly complex ones.
As you can see in this table, I tested six different configurations, starting with a small 10×10 grid containing just
2 agents and 10 goals, and scaling up to a challenging 60×60 grid with 33 agents and 169 goals. This progression allows
us to assess how well our method scales with increasing environmental complexity.
For each configuration, I ran simulations with both the proposed GNN-based approach and a baseline DQN method that
utilizes simple fully connected linear layers, to enable a direct comparison. I conducted training sessions of different
durations—250,000 steps and 1,000,000 steps—to also understand how performance evolves with extended training.
I also compared my approach against several established methods in the field: Particle Swarm Optimization, Density-Based
Scan clustering, Greedy Search, and conventional Reinforcement Learning.
To quantify performance, I focused on two primary metrics. First, the goal collection percentage measures how many of
the available goals were successfully served by the agents. Second, the grid coverage percentage which indicates how much
of the environment was observed by the agents field of view during a session. Together, these metrics provide a
comprehensive view of both task completion and exploration efficiency.
These experiments were designed to not only demonstrate the effectiveness of the approach but also to understand its
scaling properties and identify the factors that contribute most to its performance advantages.
With these experimental parameters in place, we can now talk about how my framework performed against the baseline
DQN as well as some other traditional methods across these different configurations, focusing on both goal collection
rates and grid coverage metrics.

As shown in this figure, our approach consistently outperforms the standard DQN across all scales of complexity.
Looking first at goal achievement rates, represented by the red and green lines, we see that the GNN maintains high
performance even as the environment complexity increases, achieving as high as a 90% goal collection rate in a larger
grid with 15 agents.
In contrast, the DQN, shown by the blue and orange dotted lines, struggles significantly as the environment scales up.
Ignoring the smaller environments the best-performing configuration reaches only 42% goal collection in the same
scenario—less than half of what my approach achieves.
The performance gap is consistent when we move to the right side of the graph representing more complex environments.
This demonstrates that my approach scales substantially better with increasing complexity—a critical factor for
real-world deployment.
Looking at the grid coverage results in the lower graph, we see the GNN approach consistently achieves near-complete
coverage of the environment, maintaining above 95% coverage across all configurations. The DQN, meanwhile, drops to
only 82% coverage in the largest configurations, missing significant portions of the environment.
These results confirm that the approach not only outperforms standard deep learning methods like the DQN, but also
addresses the fundamental scalability challenges that traditional methods face in complex multi-agent scenarios.

Now, while my previous results showed the final performance metrics, this graph reveals something equally important:
how quickly agents are able to collect goals over time.
The orange line represents my GNN-based approach, while the blue line shows the standard DQN method, both in the
environment with 15 agents and 76 goals. What's immediately striking is the dramatic difference in not just the final
performance, but in the rate of goal collection.
The GNN approach demonstrates remarkably faster goal collection from the very beginning of the session. Within just the
first 50 time steps, our method has already discovered and served over 65% of the goals. By the halfway point of the
episode, around time step 100, we've collected roughly 75% of all goals.
In contrast, the standard DQN method shows a much slower collection rate, achieving only about 30% collection by time
step 100, and ultimately plateauing there.
This temporal advantage is crucial for real-world applications where energy efficiency matters. Drones have limited
battery life, so serving their objectives more quickly means they can complete their mission using less energy. It also
means this system can respond more rapidly in time-sensitive scenarios like disaster response.
The steeper slope of the curve indicates more efficient exploration and coordination among the agents. Rather than
randomly searching or duplicating efforts, the agents effectively share information through the graph structure and
prioritize high-value goals through the attention mechanism.
This result underscores not just that my method works better, but that it works faster and more efficiently—a critical
consideration for practical deployment of multi-agent systems in the field.

Beyond comparing against the baseline DQN, we also benchmarked the framework against a broader set of established
methods to validate its advantages more comprehensively.
In collaboration with Niloufar from the AI-SENDS lab here at Clemson, we also evaluated my approach against several
well-established methods in the field to provide a comprehensive assessment of its effectiveness.
This chart shows the goal achievement percentages for five different methods: conventional Reinforcement Learning,
Particle Swarm Optimization, Greedy Search, Density-Based Scan clustering, and the GNN framework—all tested in the same
environment. My method achieves the highest performance with nearly 90% of goals successfully collected. This
substantially outperforms all competing approaches, with the next best method—Greedy Search—reaching only about
82% collection rate.
Particle Swarm Optimization, a well-regarded optimization technique for multi-agent systems, achieves just over 50% goal
collection. This highlights the limitations of approaches that don't effectively leverage inter-agent communication for
coordination. What's not visible in this chart but equally significant is the efficiency metric. Our approach required
an average of just 200 steps to complete a session, compared to approximately 600 steps for the other methods. This
represents a 67% reduction in the time needed to complete the mission—a substantial efficiency gain that would translate
directly to energy savings in real drone deployments.
These results validate my approach's superiority not just against baseline deep reinforcement learning but across the
spectrum of established techniques in this field. The combination of high goal collection rate and significantly reduced
mission time demonstrates that the framework successfully addresses both the effectiveness and efficiency challenges in
multi-agent cooperation.

Next, to better understand the factors contributing to my framework's success, I conducted a series of ablation studies
examining different aspects of agent communication. First I looked at how the number of communication links affects performance.
This graph shows the goal achievement percentage as I adjust the maximum number of other agents each drone can communicate with.
Starting with only 2 connections per agent, we still achieve a respectable 74% goal collection rate and as we increase
the connection limit to 3 and then 4, performance rises steadily. This initial improvement makes intuitive sense—allowing
agents to communicate with more neighbors enables better coordination and information sharing.
Beyond 5 connections, performance plateaus and fluctuates marginally — 84% at 5, 82% at 6, 85% at 7 — with no consistent
further gains.
This demonstrates an important practical finding: there's a communication-performance tradeoff in multi-agent systems.
While more communication generally improves performance, the returns diminish beyond a certain point. In our case, 5
connections per agent appears to be the sweet spot where we get most of the performance benefit without creating
excessive communication overhead.

But what happens when the communication is imperfect? Real-world multi-agent systems must often deal with communication
latency. To simulate this condition, I modified my framework to introduce a 20% chance that agents would receive outdated
information from other agents, mimicking delays in message transmission.
As you can see from this graph, even with this significant latency issue, our framework maintained robust performance.
While the baseline condition achieved 80% goal collection, the latency condition still managed roughly 73% goal collection
compared to the standard DQN’s 34% performance.
This resilience to communication delays is particularly important for applications like disaster response or environmental
monitoring, where network infrastructure may be damaged or overloaded. The results suggest that this approach can maintain
effective coordination even when communication is not instantaneous, thanks to the GNN's ability to extract valuable
information even from slightly outdated data.


Another challenge in real-world deployment is data corruption during transmission. To test the framework's resilience to
this issue, I introduced a 20% chance that messages received from other agents would have Gaussian noise applied to them.
The results again show remarkable robustness. Despite this corruption, performance only dropped again from 80% to roughly
73%. This suggests that our framework can effectively filter out noise and maintain coordination even when communication
channels are imperfect.
This resilience can be attributed to two key factors. First, the transformer-based attention mechanism naturally prioritizes
more reliable information. Second, the integration of multiple information sources through the GNN architecture provides
a form of redundancy, allowing the system to compensate for corrupted data from any single source.

Perhaps the most severe communication issue in multi-agent systems is complete message loss. To simulate this scenario,
I tested our framework with two different levels of message dropping: 10% and 25% chance of a message being lost completely.
Surprisingly, the results show virtually no performance difference between these scenarios and the baseline—in fact,
there was a marginal improvement of approximately 1% in both cases, though this difference is statistically insignificant
as it roughly amounts to one additional goal collected on average which can be easily be attributed to the randomized
nature of the environments.
These findings directly connect to my earlier study on connection limits. As we discovered, performance begins to plateau
once agents can communicate with a certain number of neighbors, creating substantial redundancy in the network. This
redundancy apparently also renders the system highly resilient to random message loss.
This resilience has significant implications for real-world deployment. It suggests this approach can maintain full
performance even in severely compromised communication environments—such as disaster areas with damaged infrastructure
or contested environments with active communication jamming. The framework's inherent adaptability eliminates the need
for complex error correction or communication recovery mechanisms.

The final aspect of my communication study examined whether constant communication is actually necessary for optimal
performance. I tested reducing the frequency of information exchange between agents to optimize bandwidth usage and
also to encourage more reliance on agents local observations.
The intervals I tested were 3, 5, 8, 10, and 12 steps.
Interestingly, the results show that less frequent communication sometimes leads to better performance. While updating
every time step achieved 80% goal collection, updating every 10 steps resulted in roughly 85%.
However, when communication became too infrequent (every 12 steps), performance dropped to roughly 73%, suggesting I'd
crossed a critical threshold.
This counter-intuitive finding suggests that constant communication might actually create some information overload or
encourage agents to rely too heavily on others rather than making independent decisions based on their immediate surroundings.
By reducing communication frequency, we not only optimize bandwidth usage but potentially improve overall system performance
by striking a better balance between collaborative and independent decision-making.

In conclusion, I've presented an innovative framework that integrates Graph Neural Networks, transformers, and Deep
Q-Learning to enable effective multi-agent coordination in partially observable environments with limited communication.
My approach demonstrates substantial improvements over existing methods across multiple metrics. I achieved at best 90%
goal collection rate compared to just 42% for the baseline DQN. I also maintained near-complete grid coverage in all
configurations while significantly reducing the steps required per episode from around 600 to just 200 steps.
The key components that enabled these improvements include the adaptive graph construction method, which efficiently
represents agent-agent and agent-goal interactions; the transformer-based message-passing mechanism with edge-feature-enhanced
attention, which captures complex interaction patterns; and the DQN with prioritized experience replay, which optimizes
agent policies even in challenging partially observable conditions.
As my ablation studies revealed, the framework demonstrates remarkable resilience to communication impairments. The
system maintains full performance even with 25% message loss and shows minimal degradation under communication latency
and data corruption. These findings suggest that this approach is particularly well-suited for real-world deployment in
challenging environments where communication reliability cannot be guaranteed.
Now, at the time of my graduation, this is where my formal research concluded. But over the past few months, just out of
personal curiosity, I revisited something that had been nagging at me — a theoretical limitation I suspected in the
Q-learning approach itself.
The core issue is that as you add more agents, the replay buffer becomes increasingly problematic. Because it stores
experiences from all agents mixed together, those experiences quickly go stale — by the time they're replayed for
training, the other agents have already updated their behavior, so the
stored data no longer reflects the current environment. This is a known non-stationarity problem in multi-agent
reinforcement learning, and it compounds as agent count grows.
So just for fun, I swapped out the DQN for an on-policy learning algorithm called MAPPO — Multi-Agent Proximal Policy
Optimization — which eliminates the replay buffer entirely. Instead of storing and replaying old data, it collects
fresh experiences under the current policy and uses them immediately.
Early results on the same 8-agent, 31×31 environment are promising — I'm seeing 98 to 100% goal collection on average.
That said, I only have a home computer to work with right now, so I haven't been able to test it at the larger scales
I evaluated the main framework on. But it's an encouraging direction.
More formally, a natural next step for the broader framework would be incorporating a visual transformer to process raw
sensor data directly such as a camera or lidar, removing the reliance on coordinate inputs entirely and making the system
more applicable to real-world deployment.

Some quick acknowledgments.
Of course the AI-SENDS lab here at Clemson with Dr. Razi which hosted the environment that directly supported and guided
my efforts on this project.
Also the Lincoln Laboratory, specifically Dr. Amin’s group who welcomed me into their lab over the summer and allowed
me access to all of the resources available there to support my research.
And of course all of the attendees of our regular research meetings as well as fellow graduate student Niloufar who all
helped prepare my research for publication and provide valuable insight.
