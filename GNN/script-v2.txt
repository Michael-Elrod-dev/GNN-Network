In recent years, we've seen increasing deployment of autonomous drone fleets for many different applications like
disaster response, environmental monitoring, and surveillance.
But effectively coordinating these multi-drone systems present several significant challenges.
First, these drones often operate under partial observability - they can only see what's within their limited field
of view, making it difficult to develop a complete understanding of the environment.
This is particularly challenging when the location of their objective isn't known in advance, for example,
in many search and rescue efforts.
Second, drones have strict energy constraints which not only limit their run time, but can also limit
their communication.
They can't simply broadcast high-resolution data to every other drone in the fleet.
Instead, they must rely on lightweight communication protocols with other nearby drones, or by utilizing
some centralized communication hub, which may not be available.
Third, these drones often operate in uncertain and dynamic environments where conditions can change,
presenting unique challenges.
Traditional path-planning algorithms like traveling salesman problem solvers and greedy algorithms struggle
under these constraints, particularly when prior information about the environment or objective locations
aren't available.
This is the gap my research addresses with a machine learning framework that integrates a Graph Neural Network,
an attention transformer, and deep reinforcement learning to enable coordination even under these conditions.

To further break down why existing path-planning methods struggle with this problem:
First, we have Traveling Salesman Problem or TSP solvers.
While mathematically elegant, they assume complete knowledge of all target locations in advance.
In real disaster scenarios or environmental monitoring missions, this information simply isn't available beforehand.
Numerical optimization techniques face similar challenges.
They can compute optimal paths efficiently, but require well-defined objectives and constraints that are often
impossible to formulate in uncertain environments.
Greedy algorithms are also computationally light and can work well for single agents, but they struggle with
multi-agent coordination.
When multiple drones make locally optimal decisions without coordination, we see significant path redundancy
and a lack of exploration.
Even standard Reinforcement Learning approaches face difficulties in multi-agent settings.
They often scale poorly as the number of agents increase and struggle to leverage the benefits of coordination
when communication is limited.
For this solution I needed an approach that could all of these conditions: partial observability, enable
lightweight information exchange between agents, and adapt to unknown environments while maintaining scalability.
This is what motivated the integrated GNN-Transformer-Deep learning framework.

So, at its core, my approach models the multi-agent system as a bidirectional graph where both agents and
objectives are represented as nodes, and their relationships as weighted edges.
This graph structure is constantly updated as agents move and explore the environment.
The Graph Neural Network serves as the foundation, enabling agents to share information with each other
through a message-passing mechanism.
Through this message-passing mechanism, each drone shares its own position along with the positions of
the objectives it's currently tracking.
When it receives messages from neighboring drones, it gains access to those drones' positions and the
objectives they've discovered — effectively expanding each drone's situational awareness beyond its own field of view.
This is enhanced with a transformer layer that allows the network to calculate the importance of information
being shared.
This allows drones to focus on objectives and important coordination opportunities while filtering out
redundant information.
Finally, the agent policy is optimized using a Deep Q-Network with a replay buffer, which is used to
stabilize the training process and update the learned policy over time.
So, with these 3 components together: the GNN provides lightweight information exchange between drones,
the transformer handles prioritizing information from that shared data, and the DQN stores agent experiences
and updates the learned policy during training until it converges to an optimal solution.

Next, to evaluate my approach, I utilized a 2D, grid-based environment to simulate drone movement over
time and objective locations.
As you can see in this visualization, we have several represented by these drone icons and the objectives
that they're searching for, shown as the green dots with the gray ones being objectives that have already
been visited.
Each agent has two constraints.
First, they have a limited field of view shown by these red circles, meaning they are only aware of
objectives within this radius, creating partial observability.
Second, each drone can only communicate up to 3 other drones depending on distance to model the bandwidth limitations.
These communication links are used to form the moving, bidirectional graph that serves as the Graph
Neural Network's structure.
When an objective falls within a drone's field of view it becomes part of the shared knowledge.
However, an objective is only considered 'served' when an agent physically enters its cell, represented
by the grey dots that agents have passed over already.
At the beginning of each session, drones are distributed along the borders of the environment and the
objectives are randomly distributed throughout the interior of the grid, with their locations unknown
to the agents until discovery.
This setup requires the drones to not only explore but to coordinate with each other to find and visit
all objectives in a reasonable amount of time.

Next, I want to break down all 3 parts of the network to explain how each plays its part in the training process.
The foundation is the Graph Neural Network.
In this environment, both drones and objectives are treated as nodes in a graph, each represented by a feature
vector capturing their positions and any objectives they're currently tracking.
An initial embedding layer processes these raw features and encodes each node's entity type.
These embedded features are then passed through multi-head transformer layers with 3 attention heads —
each head focusing on different aspects of the relationships between nodes, giving the model multiple
perspectives on the same graph at once.
This is visualized in the heatmap shown here, where brighter colors indicate stronger attention weights.
For example, Agent 2 assigns the highest weight of 1.0 to objective 4, meaning it's the current highest priority target.
This is what allows each drone to determine which information from its neighbors matters most — for instance,
prioritizing a nearby drone that has just discovered a new cluster of objectives over one that's heading
to an already-known area.
The output is a rich embedding for each drone that captures both its local state and the knowledge it has
received from its neighbors through the graph.
This embedding then feeds into the Deep Q-Network.
The DQN uses two networks: an online network for action selection and a target network for stable value
estimation — a design that prevents overestimating action values, a common issue that can lead to suboptimal behavior.
Training is stabilized with a prioritized replay buffer, which preferentially samples the transitions where
the agent was most wrong, focusing learning on the most informative experiences.
Key hyperparameters include a learning rate of 0.0005, a buffer of 100,000 transitions, and a soft update
rate of 0.001 for gradually syncing the online weights to the target network.
Agents explore using an epsilon-greedy strategy, starting fully random and gradually shifting toward
learned behavior as training progresses.

To evaluate my approach at scale, I tested an array of different environment sizes.
As you can see in this table, I tested six different configurations, starting with a small 10×10 grid
containing just 2 agents and 10 objectives, and scaling up to a 60×60 grid with 35 agents and 170 objectives.
For each configuration, I ran simulations with both the proposed GNN-based approach and a DQN method that
utilizes simple fully connected linear layers, to act as a baseline for comparison.
I conducted training sessions of different durations as well, from 250,000 overall time steps to 1,000,000,
to also understand how performance changes with extended training.
To quantify performance, I focused on two primary metrics: first, the objective collection percentage, and
second, the grid coverage percentage, which shows how much of the environment was observed by the drones'
field of view during a session.

So, for the results, as shown in this figure, my GNN approach consistently outperformed the standard DQN
across all scales of complexity.
Looking first at objective visitation rates, represented by the red and green lines, we see that the GNN
maintains high performance even as the environment complexity increases, achieving as high as 90% in a
larger grid with 15 drones.
In contrast, the DQN, shown by the blue and orange dotted lines, struggles significantly as the environment scales up.
Ignoring the smaller environments, the best-performing configuration reaches only 42% in the same environment.
And this performance gap is consistent when we move to the right side of the graph, representing much larger environments.
Looking at the grid coverage results in the lower graph, we see the GNN approach consistently achieves
near-complete coverage of the environment, maintaining above 95% coverage across all configurations.
The DQN, meanwhile, drops to roughly 80% on average, missing significant portions of the environment.

Now, while those results showed the final performance metrics, this graph reveals how quickly agents are
able to visit each objective over time.
The orange line represents my GNN-based approach, while the blue line shows the standard DQN method,
both in the environment with 15 agents and 80 objectives.
The GNN approach demonstrates remarkably faster visitation from the very beginning of the session.
Within just the first 50 time steps, you can see we've already discovered and visited over 65% of the objectives.
By the halfway point of the episode, around 100 time steps, we've collected roughly 75%.
In contrast, the standard DQN method shows a much slower collection rate, achieving only about 30% overall.
Aside from the difference in overall success, this metric is crucial for real-world applications where
energy and time efficiency matters, giving value to objectives being completed faster.

Next, to better understand other aspects of the GNN framework's performance, I conducted a series of
studies placing different constraints on the drones' communication.
First, I looked at how the number of communication links affects performance.
This graph shows the objective visitation percentage as I adjust the maximum number of other drones that
can communicate with each other at one time.
Starting with only 2 connections per drone, we still achieve a respectable 74%, and as we increase the
connection limit to 3 and then 4, performance rises steadily as expected.
This initial improvement makes intuitive sense — allowing drones to receive and share more information
enables better coordination.
Beyond 5 connections, performance begins to ultimately plateau at about 84%-86%, meaning there's a
communication-performance tradeoff.
While more communication generally improves performance, the returns diminish beyond a certain point.
In this case, 5 connections appears to arguably be the sweet spot where we get the performance benefit
without creating excessive communication overhead.

But what happens when the communication is imperfect?
Real-world systems must often deal with communication latency, for example.
So, to simulate this, I modified the framework to introduce a 20% chance that drones would receive
outdated information from other drones, to mimic delays in message transmission.
As you can see from this graph, even with this latency issue, the GNN maintained performance for the most part.
While the original GNN achieved 80%, the latency condition still managed roughly 73% compared to the
original DQN's 34% performance.
I found this example to be particularly important for real-world applications, where network infrastructure
may be damaged or overloaded, showing the GNN's ability to still extract valuable information.

Another challenge in real-world deployment is data corruption during transmission.
To test the framework's resilience to this issue, I introduced a 20% chance that messages received from
other drones would have some noise applied to them.
Despite the corruption, performance only dropped again from 80% to roughly 73%.
Suggesting the GNN can effectively filter out noise and maintain coordination even when communication is imperfect.

Perhaps the most severe communication issue is complete message loss.
To simulate this, I tested the framework with two different levels of message dropping: 10% and 25%
chance of a message being lost completely.
Surprisingly, the results show no performance difference between these scenarios.
There was a marginal improvement of approximately 1% in both cases, though this difference is within a
margin of error, as it roughly amounts to one additional objective on average, which can be easily
attributed to the randomized nature of the environments.
These findings directly connect to my earlier study on connection limits.
As I mentioned, performance begins to plateau once drones can communicate with about 5-7 other drones,
as additional information just becomes redundant for local decisions.
This redundancy in information arguably also renders the system highly resilient to random message loss.
I found this study to be the most encouraging for real-world deployment, as it suggests this approach
can maintain full performance in environments with compromised communication signals.

The final aspect of my communication study examined how much communication is actually necessary for performance.
I tested reducing the frequency of information exchange between drones to study the effect this could
have on collaboration versus local decisions.
The intervals I tested were 3, 5, 8, 10, and 12 time steps, which led to a somewhat counter-intuitive finding.
The results show that less frequent communication sometimes actually leads to better performance.
While updating every time step achieved 80%, updating every 10 steps resulted in roughly 85%.
However, when communication became too infrequent, at every 12 steps, performance dropped to roughly 73%,
suggesting I'd crossed some critical threshold.
This suggests that constant communication might actually be interrupting the drones' ability to make
independent decisions based on their immediate surroundings.
At least, at the time, that was my working explanation, but revisiting the project after graduation led
me to question something deeper about the training algorithm itself.

The DQN approach, while effective, carries a fundamental assumption that breaks down in multi-agent
settings: that the environment is stationary — meaning the dynamics don't change while the agent is learning.
In a single-agent scenario that holds fine.
But here, every other drone is simultaneously part of the environment and actively updating its own behavior.
So from any one drone's perspective, the world it's operating in is constantly shifting as its teammates
evolve — I learned that this is known as the non-stationarity problem in multi-agent reinforcement learning.
The replay buffer compounds it directly.
It stores experiences from throughout training, but by the time those experiences are sampled again,
the other agents have already changed.
The network ends up training on a world that no longer exists.
And the more agents you add, the faster every stored experience goes stale — so the problem compounds with scale.

So, a few months ago, just out of personal curiosity, I replaced the DQN with an algorithm called
Multi-Agent Proximal Policy Optimization.
This algorithm is on-policy, meaning it never stores old experiences at all.
It collects fresh data under the current policy, uses it to update the network, then discards it —
so stale experiences are impossible by design.
It also introduces what's called a centralized critic during training.
While each drone still makes its own decisions using only its local information, the component used to
evaluate those decisions during training has access to all agents' observations at once.
This gives the learning process a global view of the joint behavior, producing much more grounded and
stable updates — while keeping the system fully decentralized at runtime.
The GNN architecture is completely unchanged — only the training algorithm was replaced.
Early results on the same 8-agent, 30×30 environment are very encouraging — objective visitation reached
98 to 100% on average, compared to the 85-90% range from the original DQN.
This is personal work on a home computer, so large-scale validation hasn't really been possible yet,
but as a proof of concept, I think it's a strong signal.

So, in conclusion, I've presented a machine learning framework that integrates Graph Neural Networks,
transformers, and Deep Q-Learning to enable multi-agent coordination in partially observable environments
with limited communication.
My approach demonstrates up to 90% objective collection, compared to 42% for the baseline DQN,
maintaining near-complete grid coverage across all configurations.
The framework also showed strong resilience to real-world communication impairments, maintaining
performance even under 25% message loss, latency, and data corruption.
And as the preliminary Proximal Policy Optimization results suggest, the ceiling on performance may be higher still.
Replacing the training algorithm alone — with no changes to the GNN — pushed results to near-perfect
in the same environment.
A natural next step is validating that at scale, and longer term, incorporating raw sensor inputs like
camera or lidar data to remove the reliance on coordinate observations entirely, making the system
more applicable to real-world deployment.

Some quick acknowledgments.
Of course the AI-SENDS lab at Clemson University with Dr. Razi, who directly supported and guided
my efforts on this project.
Also the Lincoln Laboratory, specifically Group 65, who welcomed me over the summer in 2024 and
allowed me access to all of the resources available there to support my research.
And of course all of the attendees of our regular research meetings as well as fellow graduate student
Niloufar, who all helped prepare my research for publication and provide valuable insight.
